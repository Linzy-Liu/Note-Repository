\chapter{Linear Regression and Classification}

Before the discussion of Linear Regression, we have to make a rigorous description of the concept of supervised learning. The neccesssary notations along with their terms will be listed here:
The input, which is usually called \textbf{features}, will be denoted by $x^{(i)}$, and the output \textbf{target} will be denoted by $y^{(i)}$. Refering to the definition of supervosed learning,
a given pair $(x^{(i)},y^{(i)})$ is a \textbf{training example}. Then the finite set of different training example will be called \textbf{training set}. Let $\mathcal{X}$ denote the space of input,
and $\mathcal{Y}$ denote the set of output. 

Now the learning problem can be expressed in such way: Taking advantage of the training set, we want to learn a ``good'' mapping $h$ so that $h(x)$ could predict $y$ well. And such $h$ is called
\textbf{hypothesis}. And our task is to design some algorithm to take advantage of the training set effectively. 

For a linear regression, we tend to find a hypothesis with the form similar to affine functions:
\[h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2\]
To make it more simply and more generalized, we will give the form below
\[h(x)=\theta^Tx\]
where the vectors $x = (1,x_1,\ldots,x_n)$, and \textbf{weight} $\theta = (\theta_0, \theta_1,\ldots, \theta_n)$.

Obviously, when the form $h$ is clear, we'll raise a question, which kind of mapping can be seen as a ``good'' mapping? A measure is needed with no doubt. And such measure is called \textbf{cost~funtion}.
In this chapter, the cost function will be defined as 
\[J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(h(x^{(i)})-y^{(i)})^2\]
which give rise to the ordinary least square regression model. WIth cost function in hand, we now know what kind of functions is needed and how to measure their quality. The last thing we need to do is
to calcute the best weight vector under the given measure. 

\section{LMS algorithm}

Considering the \textbf{gradient~decent} algorithm, which starts with some initial $\theta$, and repeatedly performs the updates 
\begin{equation}
    \label{LR: LMS rule}
    \theta_j := \theta_j - \alpha\frac{\partial J}{\partial \theta_j}
\end{equation}
where $\alpha$ denotes the \textbf{learning~rate}. Such algorithm is simple and natural. Given the cost function, the update rule \ref{LR: LMS rule} can be calcutated specifically
\[\theta_j:=\theta_j+\alpha\sum_{i=1}^{n}(y^{(i)}-h(x^{(i)}))x^{(i)}_j \]
In a slightly more succinct way, we could write it in the vector form
\begin{equation}
    \label{LR: batch}
    \theta := \theta + \alpha\sum_{i=1}^{n}(y^{(i)}-h(x^{(i)}))x^{(i)}
\end{equation}
The rule is called \textbf{LMS} update rule(LMS stands for ``least mean square''), and is also known as \textbf{Widrow-Hoff} learning rule. Such method looks at every example in the entire training
set on every step, and is also called \textbf{batch gradient decent}. On a regular basis, similar to the discussion on Newton iteration method, the convergence of such update rule is needed to be 
considered. Given that $J$ is a convex quadratic function, the optimization problem we have posed here has only one global, and no other local, optima. Thus for a not too large learning rate $\alpha$,
the gradient decent always converges.  
In application, batch gradient decent always comes up with a problem, the update will be rather costly when the size of training set $n$ goes large. Therefore, we try to seperate it into many sub-problems,
\begin{equation}
    \label{LR: stochastic}
    \theta := \theta + \alpha(y^{(i)}-h(x^{(i)}))x^{(i)}, \text{ for $i$ in }1,\ldots,n
\end{equation}
and such rule is called \textbf{stochastic gradient decent}. There is no difference between the form of \ref{LR: batch} and \ref{LR: stochastic}, however, their process are totally different. The definition
of hypothesis $h(x)$ implies that $h(x)$ changes whenever the features changes, which means that such seperation makes sense. Although there's no equivalence between stochastic gradient decent and
batch gradient decent, the former one can finally get a reasonably good approximation to the true minimum.

\small By slowly letting the learning rate $\alpha$ decrease to zero as the algorithm runs, it is also possible to ensure that the parameters will converge to the global minimum rather than merely
oscillate around the minimum.

\section{Normal Equations}

\subsection{Matrix Derivative}
At first, we want to introduce an operation $\nabla $, which denotes the derivative of a real-valued function $f: \mathbb{R}^{n\times m} \rightarrow \mathbb{R}$ as 
\[\nabla_A f(A) := \left[
\begin{array}{cccc}
    \frac{\partial f}{\partial a_{11}} & \frac{\partial f}{\partial a_{12}} & \ldots & \frac{\partial f}{\partial a_{1m}}\\
    \frac{\partial f}{\partial a_{21}} & \frac{\partial f}{\partial a_{22}} & \ldots & \frac{\partial f}{\partial a_{2m}}\\
    \vdots & \vdots & & \vdots\\
    \frac{\partial f}{\partial a_{n1}} & \frac{\partial f}{\partial a_{n2}} & \ldots & \frac{\partial f}{\partial a_{nm}}
\end{array}\right]
\]
where $A$ is a $n \times m$ matrix with the element located in row $i$ column $j$ denoted by $a_{ij}$.

To make the illustrations clear, we will take a brief look at the property of matrix derivative.
\begin{prop}
    Suppose $A$ is a $1 \times m$ matrix, and $x \in \mathbb{R}^m$.Then
    \[\nabla_x Ax = A^T,\quad\nabla_x x^T(A^TA)x = 2A^TAx\]
\end{prop}

\subsection{Least Square Revisit}

Now we want to write $J(\theta)$ in the form of linear algerbra. For a given training set, we define the \textbf{design matrix} as a $n\times (m+1)$ matrix. 
\[X = \left[
    \begin{array}{cc}
        1 & (x^{(1)})^T \\
        1 & (x^{(2)})^T \\
        \vdots & \vdots\\
        1 & (x^{(n)})^T
    \end{array}
\right]\]
and the output can be also written as 
\[ y = \left[
    \begin{aligned}
        & y^{(1)}\\
        & y^{(2)}\\
        & \vdots\\
        & y^{(n)}
    \end{aligned}
\right]\]
Hence, the error can be written as $h_\theta(x) - y = X\theta - y$, that is to say 
\[J(\theta) = \frac{1}{2}(X\theta - y)^T(X\theta - y)\]

Therefore, we could try to minimize $J(\theta)$ by matrix derivative.
\begin{align*}
    \nabla_\theta J(\theta) &= \nabla_\theta \frac{1}{2}(X\theta - y)^T(X\theta - y)\\
                            &= \nabla_\theta \frac{1}{2}(\theta^TX^TX\theta - 2(X\theta)^Ty + y^Ty)\\
                            &= X^TX\theta - X^Ty
\end{align*}
Finally we set this derivative to zero and obtain the \textbf{normal equation}
\begin{equation}
    \label{LR: normeq}
    X^TX\theta = X^Ty
\end{equation}

Under \textbf{Maximum Likelihood Estimation}, the least-squares regression can be justified as a very natural method. Such natual comes from the consistency between the target function of these
two different methods. And let us take a brief look at the process of MLE. We try to assume that for a given $i$, the output $y^{(i)}$ obeys the Gaussian distribution whose mean is given by
$\theta^Tx^{(i)}$. To make it more natural, we further assume that these outputs are distributed iid. Then the likelihood function $L(\theta)$ can be calculated easily.

\section{Locally weighted linear regression}

In this section, we will try to discuss a \textbf{non-parametric} algorithm, LWR(Locally weighted linear regression). The concept of parametric can be illustrated as ``Once fitted, no more training''.
In contrast, it is neccesssary for the collection of non-parametric algorithm to keep the training set around when making predictions. To speak clearly, the target function will be listed here.
\begin{equation}
    \label{LR: LWR}
    \sum_{i=1}^{n}w^{(i)}(\theta^Tx^{(i)}-y^{(i)})^2
\end{equation}
where $w^{(i)}$ are non-negative values \textbf{weights}. A fairly standard choice for the weights is 
\[w^{(i)}=\exp\left(-\frac{(x-x^{(i)})^T(x-x^{(i)})}{2\tau^2}\right)\]
Note that the point $x$ is the paticular one which we are trying to predict. And such weight has fairly close dependency on $x$. The parameter $\tau$, which is called \textbf{bandwidth}, controls
how quickly the weight decreases as the distance between $x^{(i)}$ and $x$ increases.

\section{Classification}

Classification problem is a family of problems which we want to predict discrete output $y$ according to input $x$. Considering its complexity, we will begin with
\textbf{binary~classification~problem}, which implies that $y$ has only two probable values:0 and 1.In classification problem, given input $x^{(i)}$, the corresponding $y^{(i)}$ is also called
\textbf{label}.

\begin{define}[Sigmoid Function]
    Let $\sigma(x)$ denote the Sigmoid function, then:
    \[\sigma(x)=\frac{1}{1+e^{-x}}\]
\end{define}

\subsection{Logistic Regression}
Let the hypothesis in Logistic regression be $h_\theta(x) = g(\theta^Tx)$. It receives an input $x$ and gives the probability of $y=1$. Therefore, we can apply the MLE to the optimization of 
feature $\theta$. Similar to stochastic gradient ascent, the update rule of Logistic regression is 
\[\theta_j := \theta_j + \alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]
Both of them holds an indentical form $\theta := \theta + \alpha\nabla l(\theta)$.

\subsection{Fisher Scoring}

\textbf{Fisher Scoring} focus on the same hypothesis as Logistic regression, $h_\theta(x) = g(\theta^Tx)$. Hence they have identical likelihood functions. Now, we want to find another method to
optimize the likelihood function. And we have found that Newton iteration method could be a good choice. It can be write as 
\[\theta := \theta - \frac{l'(\theta)}{l''(\theta)}\]
When $\theta$ is a vector, we will write it as 
\[\theta:=\theta-H^{-1}\nabla_\theta l(\theta)\]
where $H$ denote the Hessi matrix of real-valued function $\theta$.  
